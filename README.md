
机器学习  
=======  
  
****  
### Author:Song  
### E-mail:Z.S.Chang@qq.com  
****  
参考自李航《统计学习方法》,使用Google Chrome与GitHub with MathJax插件组合浏览效果最佳！！！  
## 目录  
* [感知机学习算法](#感知机学习算法)  
* [k近邻法](#k近邻法)  
* [朴素贝叶斯](#朴素贝叶斯)  
* [决策树](#决策树)  
***  
---  
___  
* ### 感知机学习算法  
* #### **原始形式**  
  
|**输入**| 训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$ ，其中$x_i$∈$R^n$,$y_i$∈{$\{+1,-1\}$}，学习率$η(0<η≤1)$ |  
|:---------:|:-------------|  
|**输出**| w，b，感知机模型$f(x)=sign(w·x+b)$ |  
|**步骤** |（1）选取初始值w0，b0；<br>（2）在训练集中选取数据$(x_i,y_i)$；<br>（3）如果存在误分类点，即$y_{i}(w·x+b)≤0$，则<br>　　　　　　　　　$w=w+ηy_ix_i$<br>　　　　　　　　　$b=b+ηy_i$ <br>（4）转（2），直至训练集中没有误分类点。 |  
|**解释** | （1）目标是求参数w，b，使得损失函数：$min_{w,b}$ $L(w,b)=- \sum_{x_{i}∈M} y_{i}(w·x_i+b)$极小化。其中M为误分类点的集合。<br>（2）采用随机梯度下降法（SGD）求解，即极小化过程中不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。随机梯度下降的效率要高于批量梯度下降（batch gradient descent）<br>（3）当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w，b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面简的距离，直至超平面越过该误分类点使其被正确分类。|  
|**优缺点**|（1）在感知机模型中，只要给出的数据是线性可分的，那么我们就能证明在有限次迭代过程中，能够收敛；<br>（2）算法着重考虑是否将所有训练数据分类正确，而不考虑分的有多好，即存在无穷多个解，其解由于初值或不同的迭代顺序而可能有所不同；<br>（3）感知机主要的本质缺陷是它不能处理线性不可分问题；<br>（4）感知机因为是线性模型，所以不能表示复杂的函数，如异或，即感知机是无法处理异或问题，因为训练集线性不可分。|  
  
`为什么叫“感知机”：`感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激动时为‘是’，而未激动时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激动，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。  
* #### **对偶形式**  
|**输入** |训练数据集 $T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$，其中$x_i$∈$R^n$,$y_i$∈{$\{+1,-1\}$}，学习率$η(0<η≤1)$|  
|:---------:|:-------------|  
|**输出** |a，b，感知机模型$f(x)=sign(\sum_{j=1}^{N}a_jy_jx_j·x+b)$ ,其中$a=(a_i,a_2,...,a_N)$|  
|**步骤**|（1）选取初始值a=0，b=0；<br>（2）在训练集中选取数据$(x_i,y_i)$；<br> （3）如果存在误分类点，即$y_{i}(\sum_{j=1}^{N}a_jy_jx_j·x+b)≤0$，则<br>　　　　　　　　　　$a=a_i+η$<br>　　　　　　　　　　$b=b+ηy_i$ <br>（4）转（2），直至训练集中没有误分类点。|  
|**解释**|（1）为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，即Gram矩阵。$G_{N×N}=[x_{i}·y_{j}]$，可加快计算速度。<br> （2）由 $w=w+ηy_ix_i$，$b=b+ηy_i$， 可知，对于误分类点$(x_i, y_i)$,w, b关于该点的增量分别为$a_iy_ix_i$，$a_iy_i$，其中，$n_i$表示第i个实例点由于误分而进行更新的次数。因此最终w, b可表示为：<br>　　　　　　　　　　$w=\sum_{i=1}^{N}a_iy_ix_i$ <br>　　　　　　　　　　$b=\sum_{i=1}^{N}a_iy_i$ |  
----  
---  
* ### k近邻法  
**输入：** 训练数据集T，实例x，参数k  
  
**输出：** 实例x所属的类y  
  
**步骤：**  
  
（1）根据给定的距离度量，在训练集中T中找出与$x$最邻近的$k$个点，涵盖着$k$个点为$x$的邻域  
（2）在领域中根据分类决策规则（如多数表决）决定$x$的类别$y$  
  
**解释：**  
  
（1）给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。  
（2）分类决策规则通常采用多数表决，多数表决等价于经验风险最小化。  
（3）通常采用**交叉验证法**来选取最优的k值，k值太小，泛化误差偏大，k值取太大，泛化误差减小。  
（4）回归输出的是k个最近邻点取得**平均值**或其他。  
（5）对异常点不敏感  
  
**实现**  
* 暴力实现  
* **kd树实现**  
（1）构建kd树  
* 不断的用垂直于坐标轴的超平面将k维空间切分（一般选择方差最大者的坐标轴），构成一系列的k维超矩形区域。  
* 划分点选择该坐标轴上的中位数为划分点，划分成左右子树。  
* **终止条件**：直到子区域没有实例点  
  
（2）用kd树的最近邻搜索  
* 给定一个目标节点，根据该点各坐标轴的数据找到包含目标节点的叶节点（一个超矩形区域），以该目标节点为圆心，到该叶节点的样本实例点（每次划分是以一个样本的实例点的某个维度为切分点的，切分线经过该点）的距离为半径，得到一个超球体，最近的节点一定在该球体内部或表面（即该叶节点），然后从该节点出发依次回退到父节点，不断的查找另一边的子节点区域中是否有与超球体相交的点，更新与目标点**最近**的节点。  
* **终止条件**：直到回退到根节点，此时的最近点记为所求。  
* **将找到的最近点标记为已选，重复上面的步骤，忽略已选点，找齐k个点**。  
* 分类：投票法；回归：取均值  
* 建立kd-tree的时间复杂度为O(k*n*logn)  
  
* **球树（BallTree）实现**  
* **改进动机**：kd树把k维空间划分成一个一个超矩形，但是矩形区域的**角**很难处理（因为判断是否在其他子节点区域）。超球体很好的解决了这个问题。  
* 步骤也kd树类似（略）  
  
----  
---  
* ### 朴素贝叶斯法  
![朴素贝叶斯法](https://github.com/Changzhisong/MachineLearning/blob/master/朴素贝叶斯.jpg)  
----  
---  
* ### 决策树  
* #### **决策树学习基本算法**  
  
|**输入**| 训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$ ,特征集A|  
|:----:|:-----|  
|**输出**| 决策树T 　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　 |  
|**过程**|　　函数 TreeGenerate(D, A) <br>1　　生成结点node; <br>2　　**if** D中样本全属于同一类别C **then** <br>3　　　　将node标记为C类叶结点; **return** <br>4　　**end if** <br>5　　**if** A == ∅ **OR** D中样本在A上取值相同 **then** <br>6　　　　 将node标记为叶结点，其类别标记为D中样本数最多的类; **return** <br>7　　**end if** <br>8　　从A中选择**最优划分属性**$a_\*$; <br>9　　**for** $a_\*$ 的每一个值 $a_\*^v$ **do** <br>10　　　　为node生成一个分支;令$D_v$表示D中在$a_\*$上取值为$a_\*^v$的样本子集; <br>11　　　　**if** $D_v$ 为空 **then** <br>12　　　　　　将分支结点标记为叶结点，其类别标记为D(其父节点)中样本最多的类; **return** <br>13　　　　**else** <br>14　　　　　　以TreeGenerate($D_v$, A＼{$a_{*}$})为分支结点 <br>15　　　　**end if** <br>16 　 **end for** <br>|  
  
* **最优划分属性方法**  
（1）信息增益（ID3算法）  
* **熵**表示的随机变量的不确定性，**信息增益**则是已知该特征的信息而使得数据的不确定性减少的程度。  
* **缺点**： 采用信息增益准则会对可取值数目较多的特征有所偏好。因为对于取值数目较多的特征，更容易使得数据更“纯”，即各个分支节点中所包含的样本会更多的属于同一个类，也就是说，该特征会使不确定性（样本熵、信息熵、经验熵）减少的更多，比如极端情况：当对于某个特征，其对每个样本都有一个不同的取值，那么该特征是最大信息增益特征，因为若采用这个特征，其每个分支节点的纯度已达最大，然而，这样的决策树显然不具有泛化能力，无法对新样本进行有效预测。  
  
（2）信息增益比（C4.5）  
* 信息增益比准则对可取值数目较少的特征有所偏好，因此C4.5算法并不是直接选择信息增益率最大的候选划分特征，而是使用了一个启发式：先从候选划分特征中找出信息增益高于平均水平的属性。再从中选择信息增益率最大的。  
  
（3） 基尼指数 （CART）  
（4）最小化均方误差（回归）  
* **剪枝**  
  
（1）剪枝是决策树对付“过拟合”的主要手段，基本策略有“预剪枝”和“后剪枝”；  
（2）预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点。预剪枝不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销；  
（3）后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对非叶子结点进行考察，若将该节点对应的子数替换为叶子节点能带来决策树的泛化性能提升，则将该子树替换为叶节点。  
  
---  
---  
* ### 逻辑斯蒂回归  
*  
  
* ### SVM  
**1.几个概念**    
* **函数间隔$\gamma^{'}$：**
					$$\gamma^{'} = y(w^Tx + b)$$  
	* $|w^Tx + b|$表示点x到超平面的距离。通过观察$w^Tx + b$和y是否同号，判断分类是否正确。  
	* 对于训练集中m个样本点对应的m个函数间隔的最小值，就是整个训练集的函数间隔。函数间隔并不能正常反映点到超平面的距离，当分子成比例的增长时，分母也是成倍增长。
* **几何间隔$\gamma$：**
			$$\gamma = \frac{y(w^Tx + b)}{||w||_2} =\frac{\gamma^{'}}{||w||_2}$$
	* 几何间隔才是点到超平面的真正距离。
* **支持向量：**
	* 距离超平面最近的几个样本，满足$y_i(w^Tx_i + b)= 1$，则它们被称为支持向量。
	* 支持向量到超平面的距离为$1/||w||_2$,两个异类支持向量之间的距离为$2/||w||_2$。  
![svm](https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161124144326487-1331861308.jpg)　  

**2. 线性可分SVM（硬间隔）**
**输入：** 线性可分训练集${(x_1,y_1), (x_2,y_2), ..., (x_m,y_m),}$,其中x为n维特征向量。y为二元输出，值为1，或者-1。  
**输出：** 分离超平面的参数$w^{\*}$和$b^{\*}$和分类决策函数。  
**步骤：** 
（1）构造约束优化问题
优化函数定义为：$$max \frac{1}{||w||_2} s.t   y_i(w^Tx_i + b) \geq 1 (i =1,2,...m)$$  
由于$\frac{1}{||w||_2}$的最大化等同于$\frac{1}{2}||w||_2^2$的最小化。这样SVM的优化函数（**原始问题**）等价于：
$$min \;\; \frac{1}{2}||w||_2^2  \;\;   \;\;s.t \;\; y_i(w^Tx_i + b)  \geq 1 (i =1,2,...m)$$  
通过**拉格朗日对偶性**将优化目标转化为无约束的优化函数：$$L(w,b,\alpha) = \frac{1}{2}||w||_2^2 \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \;\;\;\; s.t\;\;\alpha_i \geq 0$$
原始问题的对偶问题是极大极小问题：
$$\underbrace{max}_{\alpha_i \geq 0} \;\underbrace{min}_{w,b}\;  L(w,b,\alpha)$$  
求$\;\underbrace{min}_{w,b}\;  L(w,b,\alpha)$对$\alpha$的极大，即是**对偶问题**：
$$\underbrace{min}_{\alpha} \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \bullet x_j) -  \sum\limits_{i=1}^{m} \alpha_i $$ $$s.t. \; \sum\limits_{i=1}^{m}\alpha_iy_i = 0 $$ $$ \alpha_i \geq 0  \; \;i=1,2,...m $$  
（2）通过序列最小最优化算法（**SMO算法**）求对偶优化问题的$\alpha$向量的解$\alpha^{*}$。
（3）计算$w^{*} = \sum\limits_{i=1}^{m}\alpha_i^{*}y_ix_i$  
（4）找出所有的支持向量，假设有S个，即满足$\alpha_s > 0$对应的样本$(x_s,y_s)$，通过 $y_s(\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s+b) = 1$，同时根据$y^2_s=1$，计算出每个支持向量$(x_x, y_s)$对应的$b_s^{*}$，计算出这些$b_s^{*} = y_s - \sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s$。所有的$b_s^{*}$对应的平均值即为最终的$b^{*} = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{*}$  
   （5）最终的分类超平面为：$w^{*} \bullet x + b^{*} = 0$，最终的分类决策函数为：$f(x) = sign(w^{*} \bullet x + b^{*})$    
**解释：**
（1）数据集必须线性可分。  
（2）采用拉格朗日对偶性求解对偶问题的优点:  
	　　　	A. 对偶问题往往更容易求解  
　　　		B.	自然引入核函数，进而推广到非线性分类问题  
（3）**KKT条件**：  
　　　A.对参数（w、b）求导等于0；  
　　　B.约束条件；  
　　　C.拉格朗日乘子大于等于0；  
　　　D.对应的拉格朗日乘子乘以约束条件等于0  。
（4）训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。


* **线性SVM（软间隔）**
**动机：** A.假如训练集中有一些异常点，将这些异常点去掉后，剩下的大部分都是线性可分的，此时不能使用硬间隔最大化来求超平面了。B.很难确定合适的核函数使线性可分。
 **输入：** 

　（1）选择一个惩罚系数$C>0$, 构造约束优化问题
对训练集里面的每个样本$(x_i,y_i)$引入了一个松弛变量$\xi_i \geq 0$,使得：
$$y_i(w\bullet x_i +b) \geq 1- \xi_i$$ 
同时对每一个松弛变量$\xi_i$, 支付一个代价$\xi_i$，得到**软间隔原始问题**：
$$min\;\; \;\;\;\;\frac{1}{2}||w||_2^2 +C\sum\limits_{i=1}^{m}\xi_i \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$$ $$ s.t. \;\;\;\;\; \;\; y_i(w^Tx_i + b)  \geq 1 - \xi_i \;\;(i =1,2,...m)$$ $$\xi_i \geq 0 \;\;(i =1,2,...m)\;\;\;\;\;\;\;\;\;\;\;\;$$ 
这里,$C>0$为惩罚参数，为协调两者关系的正则化惩罚系数。
将软间隔最大化的约束问题用拉格朗日函数转化为无约束问题如下：$$L(w,b,\xi,\alpha,\mu) = \frac{1}{2}||w||_2^2 +C\sum\limits_{i=1}^{m}\xi_i - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] - \sum\limits_{i=1}^{m}\mu_i\xi_i $$ 
其中 $\mu_i \geq 0, \alpha_i \geq 0$,均为拉格朗日系数。 对偶问题是拉格朗日函数的**极大极小问题**：
$$ \underbrace{ min }_{\alpha} \;\;\;\;\; \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum\limits_{i=1}^{m}\alpha_i $$ $$ s.t. \;\;\;\;\;\; \sum\limits_{i=1}^{m}\alpha_iy_i = 0\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$$ $$0 \leq \alpha_i \leq C\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$$ 
和线性可分SVM相比，仅仅是多了一个约束条件$0 \leq \alpha_i \leq C$。

（2）通过序列最小最优化算法（**SMO算法**）求对偶优化问题的$\alpha$向量的解$\alpha^{*}$。  
（3）计算$w^{*} = \sum\limits_{i=1}^{m}\alpha_i^{*}y_ix_i$
 （4）找出所有的支持向量，假设有S个，即满足$\alpha_s > 0$对应的样本$(x_s,y_s)$，通过 $y_s(\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s+b) = 1$，同时根据$y^2_s=1$，计算出每个支持向量$(x_x, y_s)$对应的$b_s^{*}$，计算出这些$b_s^{*} = y_s - \sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s$。所有的$b_s^{*}$对应的平均值即为最终的$b^{*} = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{*}$  
（5）最终的分类超平面为：$w^{*} \bullet x + b^{*} = 0$，最终的分类决策函数为：$f(x) = sign(w^{*} \bullet x + b^{*})$
**解释：**
（1）另一种解释如下：$$\underbrace{ min}_{w, b}\sum\limits_{i=1}^{m}[1-y_i(w \bullet x_i + b)]_{+} + \lambda ||w||_2^2$$　 
其中$L(y(w \bullet x + b)) = [1-y_i(w \bullet x + b)]_{+}$称为合页损失函数(hinge loss function)，下标+表示为： 
 $$ [z]_{+}=\begin{cases}z ; &{z >0}\\0;& {z\leq 0}\end{cases}$$  
也就是说，如果样本点$(x_i,y_i)$被正确分类，且函数间隔$y(w \bullet x + b)$大于1时，损失是0，否则损失是$1-y(w \bullet x + b)$,如下图中的绿线。我们在下图还可以看出其他各种模型损失和函数间隔的关系：对于0-1损失函数，如果正确分类，损失是0，误分类损失1， 如下图黑线，可见0-1损失函数是不可导的。对于感知机模型，感知机的损失函数是$[-y_i(w \bullet x + b)]_{+}$，这样当样本被正确分类时，损失是0，误分类时，损失是$-y_i(w \bullet x + b)$，如下图紫线。对于逻辑回归之类和最大熵模型对应的对数损失，损失函数是$log[1+exp(-y(w \bullet x + b))]$, 如下图红线所示。 
![合页损失函数](https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161125140636518-992065349.png)
