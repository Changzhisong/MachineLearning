机器学习  
=======  
  
****  
### Author:Song  
### E-mail:Z.S.Chang@qq.com  
****  
参考自李航《统计学习方法》,使用Google Chrome与GitHub with MathJax插件组合浏览效果最佳！！！  
## 目录  
* [感知机学习算法](#感知机学习算法)  
* [k近邻法](#k近邻法)  
* [朴素贝叶斯](#朴素贝叶斯)  
* [决策树](#决策树)  
***  
---  
___  
* ### 感知机学习算法  
* #### **原始形式**  
  
|**输入**| 训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$ ，其中$x_i$∈$R^n$,$y_i$∈{$\{+1,-1\}$}，学习率$η(0<η≤1)$ |  
|:---------:|:-------------|  
|**输出**| w，b，感知机模型$f(x)=sign(w·x+b)$ |  
|**步骤** |（1）选取初始值w0，b0；<br>（2）在训练集中选取数据$(x_i,y_i)$；<br>（3）如果存在误分类点，即$y_{i}(w·x+b)≤0$，则<br>　　　　　　　　　$w=w+ηy_ix_i$<br>　　　　　　　　　$b=b+ηy_i$ <br>（4）转（2），直至训练集中没有误分类点。 |  
|**解释** | （1）目标是求参数w，b，使得损失函数：$min_{w,b}$ $L(w,b)=- \sum_{x_{i}∈M} y_{i}(w·x_i+b)$极小化。其中M为误分类点的集合。<br>（2）采用随机梯度下降法（SGD）求解，即极小化过程中不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。随机梯度下降的效率要高于批量梯度下降（batch gradient descent）<br>（3）当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w，b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面简的距离，直至超平面越过该误分类点使其被正确分类。|  
|**优缺点**|（1）在感知机模型中，只要给出的数据是线性可分的，那么我们就能证明在有限次迭代过程中，能够收敛；<br>（2）算法着重考虑是否将所有训练数据分类正确，而不考虑分的有多好，即存在无穷多个解，其解由于初值或不同的迭代顺序而可能有所不同；<br>（3）感知机主要的本质缺陷是它不能处理线性不可分问题；<br>（4）感知机因为是线性模型，所以不能表示复杂的函数，如异或，即感知机是无法处理异或问题，因为训练集线性不可分。|  
  
`为什么叫“感知机”：`感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激动时为‘是’，而未激动时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激动，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。  
* #### **对偶形式**  
|**输入** |训练数据集 $T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$，其中$x_i$∈$R^n$,$y_i$∈{$\{+1,-1\}$}，学习率$η(0<η≤1)$|  
|:---------:|:-------------|  
|**输出** |a，b，感知机模型$f(x)=sign(\sum_{j=1}^{N}a_jy_jx_j·x+b)$ ,其中$a=(a_i,a_2,...,a_N)$|  
|**步骤**|（1）选取初始值a=0，b=0；<br>（2）在训练集中选取数据$(x_i,y_i)$；<br> （3）如果存在误分类点，即$y_{i}(\sum_{j=1}^{N}a_jy_jx_j·x+b)≤0$，则<br>　　　　　　　　　　$a=a_i+η$<br>　　　　　　　　　　$b=b+ηy_i$ <br>（4）转（2），直至训练集中没有误分类点。|  
|**解释**|（1）为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，即Gram矩阵。$G_{N×N}=[x_{i}·y_{j}]$，可加快计算速度。<br> （2）由 $w=w+ηy_ix_i$，$b=b+ηy_i$， 可知，对于误分类点$(x_i, y_i)$,w, b关于该点的增量分别为$a_iy_ix_i$，$a_iy_i$，其中，$n_i$表示第i个实例点由于误分而进行更新的次数。因此最终w, b可表示为：<br>　　　　　　　　　　$w=\sum_{i=1}^{N}a_iy_ix_i$ <br>　　　　　　　　　　$b=\sum_{i=1}^{N}a_iy_i$ |  
----   
---
* ### k近邻法    
**输入：** 训练数据集T，实例x，参数k

**输出：** 实例x所属的类y

**步骤：**

（1）根据给定的距离度量，在训练集中T中找出与_x_最邻近的k个点，涵盖着k个点为x的邻域
（2）在领域中根据分类决策规则（如多数表决）决定_x_的类别_y_

**解释：**

（1）给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。
（2）分类决策规则通常采用多数表决，多数表决等价于经验风险最小化。
（3）通常采用**交叉验证法**来选取最优的k值，k值太小，泛化误差偏大，k值取太大，泛化误差减小。  
（4）回归输出的是k个最近邻点取得**平均值**或其他。
（5）对异常点不敏感

**实现**
 * 暴力实现
 * **kd树实现**
（1）构建kd树
	* 不断的用垂直于坐标轴的超平面将k维空间切分（一般选择方差最大者的坐标轴），构成一系列的k维超矩形区域。
	* 划分点选择该坐标轴上的中位数为划分点，划分成左右子树。
	* **终止条件**：直到子区域没有实例点 
	
	（2）用kd树的最近邻搜索
	*	给定一个目标节点，根据该点各坐标轴的数据找到包含目标节点的叶节点（一个超矩形区域），以该目标节点为圆心，到该叶节点的样本实例点（每次划分是以一个样本的实例点的某个维度为切分点的，切分线经过该点）的距离为半径，得到一个超球体，最近的节点一定在该球体内部或表面（即该叶节点），然后从该节点出发依次回退到父节点，不断的查找另一边的子节点区域中是否有与超球体相交的点，更新与目标点**最近**的节点。
	*	**终止条件**：直到回退到根节点，此时的最近点记为所求。
	*	**将找到的最近点标记为已选，重复上面的步骤，忽略已选点，找齐k个点**。
	*	分类：投票法；回归：取均值
	*	建立kd-tree的时间复杂度为O(k*n*logn)
	
 * **球树（BallTree）实现**
	* **改进动机**：kd树把k维空间划分成一个一个超矩形，但是矩形区域的**角**很难处理（因为判断是否在其他子节点区域）。超球体很好的解决了这个问题。
	* 步骤也kd树类似（略）
	
----  
---
 * ### 朴素贝叶斯法  
![朴素贝叶斯法](https://github.com/Changzhisong/MachineLearning/blob/master/朴素贝叶斯.jpg)  
----  
---
 * ### 决策树  
	* #### **决策树学习基本算法**  
  
|**输入**| 训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$ ,特征集A|  
|:----:|:-----|  
|**输出**| 决策树T 　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　 |  
|**过程**|　　函数 TreeGenerate(D, A) <br>1　　生成结点node; <br>2　　**if** D中样本全属于同一类别C **then** <br>3　　　　将node标记为C类叶结点; **return** <br>4　　**end if** <br>5　　**if** A == ∅ **OR** D中样本在A上取值相同 **then** <br>6　　　　 将node标记为叶结点，其类别标记为D中样本数最多的类; **return** <br>7　　**end if** <br>8　　从A中选择**最优划分属性**$a_\*$; <br>9　　**for** $a_\*$ 的每一个值 $a_\*^v$ **do** <br>10　　　　为node生成一个分支;令$D_v$表示D中在$a_\*$上取值为$a_\*^v$的样本子集; <br>11　　　　**if** $D_v$ 为空 **then** <br>12　　　　　　将分支结点标记为叶结点，其类别标记为D(其父节点)中样本最多的类; **return** <br>13　　　　**else** <br>14　　　　　　以TreeGenerate($D_v$, A＼{$a_{*}$})为分支结点 <br>15　　　　**end if** <br>16 　 **end for** <br>|  

 *	**最优划分属性方法**  
	 （1）信息增益（ID3算法）
	 * **熵**表示的随机变量的不确定性，**信息增益**则是已知该特征的信息而使得数据的不确定性减少的程度。
	*	**缺点**： 采用信息增益准则会对可取值数目较多的特征有所偏好。因为对于取值数目较多的特征，更容易使得数据更“纯”，即各个分支节点中所包含的样本会更多的属于同一个类，也就是说，该特征会使不确定性（样本熵、信息熵、经验熵）减少的更多，比如极端情况：当对于某个特征，其对每个样本都有一个不同的取值，那么该特征是最大信息增益特征，因为若采用这个特征，其每个分支节点的纯度已达最大，然而，这样的决策树显然不具有泛化能力，无法对新样本进行有效预测。
	
	（2）信息增益比（C4.5）
	 * 信息增益比准则对可取值数目较少的特征有所偏好，因此C4.5算法并不是直接选择信息增益率最大的候选划分特征，而是使用了一个启发式：先从候选划分特征中找出信息增益高于平均水平的属性。再从中选择信息增益率最大的。  
	 
	（3） 基尼指数 （CART）
	（4）最小化均方误差（回归）
* **剪枝**

	（1）剪枝是决策树对付“过拟合”的主要手段，基本策略有“预剪枝”和“后剪枝”；
	（2）预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点。预剪枝不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销；
	（3）后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对非叶子结点进行考察，若将该节点对应的子数替换为叶子节点能带来决策树的泛化性能提升，则将该子树替换为叶节点。

---
---  
* ### 逻辑斯蒂回归 
*
  
* ### SVM  
1. 函数间隔与几何间隔  
在正式介绍SVM的模型和损失函数之前，我们还需要先了解下函数间隔和几何间隔的知识。  
在分离超平面固定为$w^Tx + b = 0$的时候，$|w^Tx + b&nbsp;|$表示点x到超平面的距离。通过观察$w^Tx + b $和y是否同号，我们判断分类是否正确，这些知识我们在感知机模型里都有讲到。这里我们引入函数间隔的概念，定义函数间隔$\gamma^{'}$为：$$\gamma^{'} = y(w^Tx + b)$$  
　　　　可以看到，它就是感知机模型里面的误分类点到超平面距离的分子。对于训练集中m个样本点对应的m个函数间隔的最小值，就是整个训练集的函数间隔。  
　　　　函数间隔并不能正常反应点到超平面的距离，在感知机模型里我们也提到，当分子成比例的增长时，分母也是成倍增长。为了统一度量，我们需要对法向量$w$加上约束条件，这样我们就得到了几何间隔$\gamma$,定义为：$$\gamma = \frac{y(w^Tx + b)}{||w||_2} = &nbsp;\frac{\gamma^{'}}{||w||_2}$$  
　　　　几何间隔才是点到超平面的真正距离，感知机模型里用到的距离就是几何距离。  
3. 支持向量  
　　　　在感知机模型中，我们可以找到多个可以分类的超平面将数据分开，并且优化时希望所有的点都离超平面远。但是实际上离超平面很远的点已经被正确分类，我们让它离超平面更远并没有意义。反而我们最关心是那些离超平面很近的点，这些点很容易被误分类。如果我们可以让离超平面比较近的点尽可能的远离超平面，那么我们的分类效果会好有一些。SVM的思想起源正起于此。  
　　　　如下图所示，分离超平面为$w^Tx + b = 0$，如果所有的样本不光可以被超平面分开，还和超平面保持一定的函数距离（下图函数距离为1），那么这样的分类超平面是比感知机的分类超平面优的。可以证明，这样的超平面只有一个。和超平面平行的保持一定的函数距离的这两个超平面对应的向量，我们定义为支持向量，如下图虚线所示。  
![svm](https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161124144326487-1331861308.jpg)　  支持向量到超平面的距离为$1/||w||_2$,两个支持向量之间的距离为$2/||w||_2$。  
4. SVM模型目标函数与优化  
　　　　SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：$$max \;\; \gamma = \frac{y(w^Tx + b)}{||w||_2}&nbsp; \;\; s.t \;\; y_i(w^Tx_i + b) = \gamma^{'(i)} \geq \gamma^{'} (i =1,2,...m)$$  
　　　　一般我们都取函数间隔$\gamma^{'}$为1，这样我们的优化函数定义为：$$max \;\; \frac{1}{||w||_2}&nbsp; \;\; s.t \;\; y_i(w^Tx_i + b) &nbsp;\geq 1 (i =1,2,...m)$$  
　　　　也就是说，我们要在约束条件$y_i(w^Tx_i + b) &nbsp;\geq 1 (i =1,2,...m)$下，最大化$\frac{1)}{||w||_2}$。可以看出，这个感知机的优化方式不同，感知机是固定分母优化分子，而SVM是固定分子优化分母，同时加上了支持向量的限制。  
　　　　由于$\frac{1}{||w||_2}$的最大化等同于$\frac{1}{2}||w||_2^2$的最小化。这样SVM的优化函数等价于：$$min \;\; \frac{1}{2}||w||_2^2&nbsp; \;\; s.t \;\; y_i(w^Tx_i + b) &nbsp;\geq 1 (i =1,2,...m)$$  
　　　　由于目标函数$\frac{1}{2}||w||_2^2$是凸函数，同时约束条件不等式是仿射的，根据凸优化理论，我们可以通过拉格朗日函数将我们的优化目标转化为无约束的优化函数，这和<a id="homepage1_HomePageDays_DaysList_ctl00_DayList_TitleUrl_0" class="postTitle2" href="http://www.cnblogs.com/pinard/p/6093948.html">最大熵模型原理小结</a>中讲到了目标函数的优化方法一样。具体的，优化函数转化为：$$L(w,b,\alpha) = \frac{1}{2}||w||_2^2 - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \; 满足\alpha_i \geq 0$$  
　　　　由于引入了朗格朗日乘子，我们的优化目标变成：$$\underbrace{min}_{w,b}\; \underbrace{max}_{\alpha_i \geq 0} L(w,b,\alpha)$$  
　　　　和最大熵模型一样的，我们的这个优化函数满足KKT条件，也就是说，我们可以通过拉格朗日对偶将我们的优化问题转化为等价的对偶问题来求解。如果对凸优化和拉格朗日对偶不熟悉，建议阅读鲍德的《凸优化》。  
　　　　也就是说，现在我们要求的是：$$\underbrace{max}_{\alpha_i \geq 0} \;\underbrace{min}_{w,b}\;&nbsp;&nbsp;L(w,b,\alpha)$$  
　　　　从上式中，我们可以先求优化函数对于$w和b$的极小值。接着再求拉格朗日乘子$\alpha$的极大值。  
　　　　首先我们来求$w和b$的极小值，即$\underbrace{min}_{w,b}\;&nbsp;&nbsp;L(w,b,\alpha)$。这个极值我们可以通过对$w和b$分别求偏导数得到：$$\frac{\partial L}{\partial w} = 0 \;\Rightarrow w = \sum\limits_{i=1}^{m}\alpha_iy_ix_i $$ $$\frac{\partial L}{\partial b} = 0 \;\Rightarrow \sum\limits_{i=1}^{m}\alpha_iy_i = 0$$  
&nbsp;　　　　从上两式子可以看出，我们已经求得了$w和\alpha$的关系，只要我们后面接着能够求出优化函数极大化对应的$\alpha$，就可以求出我们的$w$了，至于b，由于上两式已经没有b，所以最后的b可以有多个。  
　　　　好了，既然我们已经求出$w和\alpha$的关系，就可以带入优化函数$L(w,b,\alpha)$消去$w$了。我们定义:$$\psi(\alpha) = \underbrace{min}_{w,b}\;&nbsp;&nbsp;L(w,b,\alpha)$$  
　　　　现在我们来看将$w$替换为$\alpha$的表达式以后的优化函数$\psi(\alpha)$的表达式：  
&nbsp;$$ \begin{align}&nbsp;\psi(\alpha) &amp; = &nbsp;\frac{1}{2}||w||_2^2 - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \\&amp;&nbsp;= \frac{1}{2}w^Tw-\sum\limits_{i=1}^{m}\alpha_iy_iw^Tx_i - \sum\limits_{i=1}^{m}\alpha_iy_ib +&nbsp;\sum\limits_{i=1}^{m}\alpha_i \\&amp;&nbsp;= \frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i -\sum\limits_{i=1}^{m}\alpha_iy_iw^Tx_i - \sum\limits_{i=1}^{m}\alpha_iy_ib +&nbsp;\sum\limits_{i=1}^{m}\alpha_i \\&amp; = \frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - \sum\limits_{i=1}^{m}\alpha_iy_ib +&nbsp;\sum\limits_{i=1}^{m}\alpha_i &nbsp;\\&amp; = - \frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - \sum\limits_{i=1}^{m}\alpha_iy_ib +&nbsp;\sum\limits_{i=1}^{m}\alpha_i &nbsp;\\&amp; = - \frac{1}{2}w^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - b\sum\limits_{i=1}^{m}\alpha_iy_i +&nbsp;\sum\limits_{i=1}^{m}\alpha_i \\&amp; = -\frac{1}{2}(\sum\limits_{i=1}^{m}\alpha_iy_ix_i)^T(\sum\limits_{i=1}^{m}\alpha_iy_ix_i) - b\sum\limits_{i=1}^{m}\alpha_iy_i +&nbsp;\sum\limits_{i=1}^{m}\alpha_i &nbsp;\\&amp; = -\frac{1}{2}\sum\limits_{i=1}^{m}\alpha_iy_ix_i^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i - b\sum\limits_{i=1}^{m}\alpha_iy_i +&nbsp;\sum\limits_{i=1}^{m}\alpha_i \\&amp; = -\frac{1}{2}\sum\limits_{i=1}^{m}\alpha_iy_ix_i^T\sum\limits_{i=1}^{m}\alpha_iy_ix_i +&nbsp;\sum\limits_{i=1}^{m}\alpha_i \\&amp; = -\frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_iy_ix_i^T\alpha_jy_jx_j +&nbsp;\sum\limits_{i=1}^{m}\alpha_i \\&amp; = \sum\limits_{i=1}^{m}\alpha_i &nbsp;- \frac{1}{2}\sum\limits_{i=1,j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j &nbsp;\end{align}$$  
　　　　其中，(1)式到(2)式用到了范数的定义$||w||_2^2 =w^Tw$, (2)式到(3)式用到了上面的$w = \sum\limits_{i=1}^{m}\alpha_iy_ix_i$，&nbsp;(3)式到(4)式把和样本无关的$w^T$提前，(4)式到(5)式合并了同类项，(5)式到(6)式把和样本无关的$b$提前，(6)式到(7)式继续用到$w = \sum\limits_{i=1}^{m}\alpha_iy_ix_i$，（7）式到(8)式用到了向量的转置。由于常量的转置是其本身，所有只有向量$x_i$被转置，（8）式到(9)式用到了上面的$\sum\limits_{i=1}^{m}\alpha_iy_i = 0$，（9）式到(10)式使用了$(a+b+c+…)(a+b+c+…)=aa+ab+ac+ba+bb+bc+…$的乘法运算法则，（10）式到(11)式仅仅是位置的调整。  
　　　　从上面可以看出，通过对$w,b$极小化以后，我们的优化函数$\psi(\alpha)$仅仅只有$\alpha$向量做参数。只要我们能够极大化$\psi(\alpha)$，就可以求出此时对应的$\alpha$，进而求出$w,b$.  
　　　　对$\psi(\alpha)$求极大化的数学表达式如下:$$&nbsp;\underbrace{max}_{\alpha} -\frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \bullet x_j) + \sum\limits_{i=1}^{m} \alpha_i $$ $$s.t. \; \sum\limits_{i=1}^{m}\alpha_iy_i = 0 $$ $$ \alpha_i \geq 0&nbsp; \; i=1,2,...m $$  
　　　　可以去掉负号，即为等价的极小化问题如下：  
$$\underbrace{min}_{\alpha} \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \bullet x_j) - &nbsp;\sum\limits_{i=1}^{m} \alpha_i $$ $$s.t. \; \sum\limits_{i=1}^{m}\alpha_iy_i = 0 $$ $$ \alpha_i \geq 0&nbsp; \; i=1,2,...m $$  
&nbsp;　　　　只要我们可以求出上式极小化时对应的$\alpha$向量就可以求出$w和b$了。具体怎么极小化上式得到对应的$\alpha$，一般需要用到SMO算法，这个算法比较复杂，我们后面会专门来讲。在这里，我们假设通过SMO算法，我们得到了对应的$\alpha$的值$\alpha^{*}$。  
　　　　那么我们根据$w = \sum\limits_{i=1}^{m}\alpha_iy_ix_i$，可以求出对应的$w$的值$$w^{*} = \sum\limits_{i=1}^{m}\alpha_i^{*}y_ix_i$$  
　　　　求b则稍微麻烦一点。注意到，对于任意支持向量$(x_x, y_s)$，都有$$y_s(w^Tx_s+b) = y_s(\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s+b) = 1 $$  
　　　　假设我们有S个支持向量，则对应我们求出S个$b^{*}$,理论上这些$b^{*}$都可以作为最终的结果， 但是我们一般采用一种更健壮的办法，即求出所有支持向量所对应的$b_s^{*}$，然后将其平均值作为最后的结果。注意到对于严格线性可分的SVM，$b$的值是有唯一解的，也就是这里求出的所有$b^{*}$都是一样的，这里我们仍然这么写是为了和后面加入软间隔后的SVM的算法描述一致。  
　　　　怎么得到支持向量呢？根据KKT条件中的对偶互补条件$\alpha_{i}^{*}(y_i(w^Tx_i + b) - 1) = 0$，如果$\alpha_i&gt;0$则有$y_i(w^Tx_i + b) =1$ 即点在支持向量上，否则如果$\alpha_i=0$则有$y_i(w^Tx_i + b) \geq 1$，即样本在支持向量上或者已经被正确分类。  
5. 线性可分SVM的算法过程  
　　　　这里我们对线性可分SVM的算法过程做一个总结。  
　　　　输入是线性可分的m个样本${(x_1,y_1), (x_2,y_2), ..., (x_m,y_m),}$,其中x为n维特征向量。y为二元输出，值为1，或者-1.  
　　　　输出是分离超平面的参数$w^{*}和b^{*}$和分类决策函数。  
　　　　算法过程如下：  
　　　　1）构造约束优化问题$$\underbrace{min}_{\alpha} \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \bullet x_j) - &nbsp;\sum\limits_{i=1}^{m} \alpha_i $$ $$s.t. \; \sum\limits_{i=1}^{m}\alpha_iy_i = 0 $$ $$ \alpha_i \geq 0&nbsp; \; i=1,2,...m $$  
　　　　2）用SMO算法求出上式最小时对应的$\alpha$向量的值$\alpha^{*}$向量.  
　　　　3) 计算$w^{*} = \sum\limits_{i=1}^{m}\alpha_i^{*}y_ix_i$  
　　　　4) 找出所有的S个支持向量,即满足$\alpha_s &gt; 0对应的样本(x_s,y_s)$，通过&nbsp;$y_s(\sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s+b) = 1$，计算出每个支持向量$(x_x, y_s)$对应的$b_s^{*}$,计算出这些$b_s^{*} = y_s - \sum\limits_{i=1}^{m}\alpha_iy_ix_i^Tx_s$. 所有的$b_s^{*}$对应的平均值即为最终的$b^{*} = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{*}$  
&nbsp;　　　　这样最终的分类超平面为：$w^{*} \bullet x + b^{*} = 0 $，最终的分类决策函数为：$f(x) = sign(w^{*} \bullet x + b^{*})$  
　　　　  
　　　　线性可分SVM的学习方法对于非线性的数据集是没有办法使用的， 有时候不能线性可分的原因是线性数据集里面多了少量的异常点，由于这些异常点导致了数据集不能线性可分， 那么怎么可以处理这些异常点使数据集依然可以用线性可分的思想呢？ 我们在下一节的线性SVM的软间隔最大化里继续讲。
